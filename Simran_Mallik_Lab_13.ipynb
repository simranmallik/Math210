{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simran Mallik Lab 13.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simranmallik/Math210/blob/main/Simran_Mallik_Lab_13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9uexyJcv5Ym"
      },
      "source": [
        "# Lab 1.3\r\n",
        "\r\n",
        "Goals:\r\n",
        "* Compute arithmetic expressions.\r\n",
        "* Define and use functions.\r\n",
        "* Graph functions.\r\n",
        "* Implement Newton's method.\r\n",
        "* Determine which properties of a function affect the speed of Newton's method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUD-70U3GRpT"
      },
      "source": [
        "### Instructions\r\n",
        "* With your group, work through each of the questions below.  Most questions include some discussion and some bullet-pointed instructions.  Make sure you carry out the bullet-pointed instructions, because that it what is graded.  \r\n",
        "* You are expected to collaborate with your group.  You should arrive at the answers to these questions together.  However, your submitted answers should be expressed in your own words.\r\n",
        "* If you need to pause, you can save and come back to your work later.  \r\n",
        "* When you are done, select \"download .ipynb\" from the file menu.  Save the ipynb to your computer, then upload it to the Lab assignment in gradescope.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g3kL9HcwgDU"
      },
      "source": [
        "We will begin by importing the libraries we use for graphing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8wEIdO3wfws"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNOhsOpmxLyH"
      },
      "source": [
        "Define five algebraic functions by running this cell.  Each person in your group should pick a different one of these functions.  For the rest of the lab, \"your function\" refers to the function you picked. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1tu6QZFv2Hz"
      },
      "source": [
        "# Ryan\r\n",
        "def f1(x):\r\n",
        "  return x**3 - 4*x**2 + 6*x - 2.5\r\n",
        "# tries: 8, distance: .60\r\n",
        "\r\n",
        "# Afshin\r\n",
        "def f2(x):\r\n",
        "  return x**3 - 6*x**2 + 14*x - 10.5\r\n",
        "# tries: 5, distance: .12\r\n",
        "\r\n",
        "# Abbey\r\n",
        "def f3(x):\r\n",
        "  return x**3 - 10*x**2 + 30*x - 26.5\r\n",
        "# tries: 7, distance: .35\r\n",
        "\r\n",
        "def f4(x):\r\n",
        "  return x**3 - 5*x**2 + 10*x - 6.5\r\n",
        "\r\n",
        "# Simran\r\n",
        "def f5(x):\r\n",
        "  return x**3 - 8*x**2 + 22*x - 18.5\r\n",
        "# tries: 7, distance: .28"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr5nenYR0eJT"
      },
      "source": [
        "Your function has a root (x-intercept) near $x=2$, but it would be difficult to compute exactly.  Here's an idea.  We can make the linearization $L(x)$ of your function at $a=2$, and solve for the point where $L(x)=0$.  It won't be exactly the root of your function, but it should be a closer guess than $2$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-clb4YF0Z0S"
      },
      "source": [
        "### Q1\r\n",
        "\r\n",
        "* In a code cell below this, define a function `l(x)` which is the linearization of your function at $a=2$.  You can compute the derivative and value you need by hand, with a calculator, or in a code cell.\r\n",
        "* Create a graph using $x$ values near $2$ that contains your function and its linearization.  \r\n",
        "* Use algebra to solve for where $L(x)=0$ on your own paper. State your solution and check that it matches the intercept your graph shows you.\r\n",
        "\r\n",
        "You graph two functions on one graph by calling `plt.plot()` twice before you call `plt.show()`.  You can restrict the height of the graph so that you can see the details better by calling `plt.ylim((-5,5))` before you call `plt.show()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "OhkNqLt_Uz78",
        "outputId": "0e01a10d-20e4-42b5-fa70-d9ecc951e167"
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "\n",
        "a = np.arange(0,4,1)\n",
        "def l(a):\n",
        "  return 2*a - 2.5\n",
        "\n",
        "def o(a):\n",
        "  return a**3 - 8*(a**2) + 22*a - 18.5\n",
        "\n",
        "plt.plot(a, l(a))\n",
        "plt.plot(a, o(a))\n",
        "# plt.ylim((-5,5))\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0Jr9_sx12rS"
      },
      "source": [
        "### Q2\r\n",
        "\r\n",
        "The first idea of Newton's method is that the $x$-intercept of the linearization at $a$ is usually a better approximation of the root than $a$ is.  \r\n",
        "* On paper, write the linearization of your function at a general point $a$ and solve $L(x)=0$ for $x$.  \r\n",
        "* In a cell below define a function `new(a)` which returns the value of `x` you solved above for an input `a`.\r\n",
        "* Test your definition by inputting `new(2)`.  This should give the answer you got from Q1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7AJVIhyXFcL",
        "outputId": "15203d9d-5daa-4f52-931f-b909f1c08e94"
      },
      "source": [
        "# linearization = tangent line\n",
        "# x = (negative of original function / derivative of original function) + a\n",
        "def new(a):\n",
        "  x = (((-a)**3 + (8*(a**2)) - (22*a) + 18.5)/(3*(a**2) - (16*a) + 22)) + a\n",
        "  return x\n",
        "\n",
        "new(2)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RNm8ojL2hdN"
      },
      "source": [
        "### Q3\r\n",
        "\r\n",
        "If we can use the linearization at `a` to produce a better approximation, then we can use the linearization at that point to produce an even better approximation.  Then we can use the linearization at that point to produce an even better approximation, and so on and so on.  This is the second idea of Newton's method.  \r\n",
        "* Add a line to your definition of `new(a)` that prints the value of `x` before it returns `x`.  \r\n",
        "* Apply your function 10 times to the intial guess $a=2$: `new(new(new(new(new(new(new(new(new(new(2))))))))))` \r\n",
        "\r\n",
        "You should notice that the last few approximations are remarkably close together.  That's because the approximations are so accurate that is they cannot be improved upon much."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_75T-SvgyFb",
        "outputId": "6cdf02cc-0d6d-4c53-fd85-e95d2f658e0b"
      },
      "source": [
        "def new(a):\n",
        "  x = (((-a)**3 + (8*(a**2)) - (22*a) + 18.5)/(3*(a**2) - (16*a) + 22)) + a\n",
        "  print(x)\n",
        "  return x\n",
        "\n",
        "new(new(new(new(new(new(new(new(new(new(2))))))))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.25\n",
            "1.4813084112149533\n",
            "1.5253516863509085\n",
            "1.5268409691718012\n",
            "1.5268426322797213\n",
            "1.5268426322817934\n",
            "1.5268426322817925\n",
            "1.5268426322817925\n",
            "1.5268426322817925\n",
            "1.5268426322817925\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5268426322817925"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9ytTejn32gC"
      },
      "source": [
        "###Q4\r\n",
        "\r\n",
        "We should assume that the final outcome of applying the `new` function 10 times is close enough to the actual root of your function.  Now the question is, how quickly did Newton's method get you close to that actual root?  To answer this you'll want to know the error in each approximation.\r\n",
        "\r\n",
        "* Assign the actual value of the root of your function near 2 to a variable\r\n",
        "* Modify your `new` function to instead print the difference between `x` and that actual value.\r\n",
        "* Apply the `new` function 10 times again to the intial guess $a=2$.  What do you notice?\r\n",
        "\r\n",
        "It may help you make sense of what you saw to know that python encodes small numbers in scientific notation.  Furthermore, numbers in python are by default only stored to 32 decimal places.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iq9A865mhD3R",
        "outputId": "8a53104a-4edb-4b63-e7b8-e980709444d2"
      },
      "source": [
        "root = 1.5268426322817925\n",
        "\n",
        "def new(a):\n",
        "  x = (((-a)**3 + (8*(a**2)) - (22*a) + 18.5)/(3*(a**2) - (16*a) + 22)) + a\n",
        "  print(root - x)\n",
        "  return x\n",
        "\n",
        "new(new(new(new(new(new(new(new(new(new(2))))))))))\n",
        "\n",
        "# .27 away from actual root, ___ away from actual root .... "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.2768426322817925\n",
            "0.04553422106683924\n",
            "0.0014909459308840578\n",
            "1.6631099912789438e-06\n",
            "2.071232074740692e-12\n",
            "-8.881784197001252e-16\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5268426322817925"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOSltMrY5qpF"
      },
      "source": [
        "###Q5\r\n",
        "\r\n",
        "This is our main question:  \r\n",
        "\r\n",
        "> What properties of a function make Newton's method more or less efficient at finding a root?\r\n",
        "\r\n",
        "The method worked for everyone's function eventually, but some coverged more quickly than others.  They got closer to the actual value in fewer steps.  The purpose of this question is to systematically generate and compare data and to try to draw conclusions from that data.  Confer with your group and agree on an apporach to the following questions.  Report your decisions and findings in a combination of text and code cells.\r\n",
        "\r\n",
        "* How are you going to measure how quickly the method is converging?\r\n",
        "* Is everyone's root the same distance from their starting guess?  In order to get comparable data how can you control for the discrepancies here?\r\n",
        "* Generate and share the data about your respective functions.  Explain how quickly each person's function converges according the the measure you chose.\r\n",
        "* Make a conjecture about what underlying property of the functions or their graphs caused some to converge faster than others.  \r\n",
        "* For full credit, propose a reasonable mathematical explanation for why this property would have that effect.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpMHgZCtkCtk",
        "outputId": "97657e46-a92b-47f7-ad49-cdd198bb62a2"
      },
      "source": [
        "# 1. How are you going to measure how quickly the method is converging? \n",
        "  # For each person, we would measure the number of times it took before the method converged to 0, and compare the number of times. \n",
        "  # On the seventh time, the method converged to 0 for me, which was different from the number of times it took for my \n",
        "  # groupmates' methods to converge to 0. \n",
        "\n",
        "# 2. Is everyone's root the same distance from their starting guess? In order to get comparable data how can you control for the discrepancies here?\n",
        "  # Not everyone's root is the same distance from their starting guess. To get comparable data, you take the difference between\n",
        "  # the starting distance and root and compare those distances and also compare the number of times it took to get to that final distance. \n",
        "\n",
        "# 3. Generate and share the data about your respective functions. Explain how quickly each person's function converges according to the measure you chose.\n",
        "  # We could not find an exact trend in our data, however, the person with the equation that resulted in the most tries, 8 tries, to get to 0\n",
        "  # in our group also displayed the greatest distance between the initial guess and the root, which was .60. We believe that generally, the fewer the number of\n",
        "  # tries it took to get to 0, the smaller the distance between the initial guess and the root would be. For example, one person's equation\n",
        "  # resulted in 5 tries to get to 0 with a distance of 0.12. We think that it's possible that the smaller the distance is between the root and the starting guess, the faster the method gets to 0. \n",
        "  \n",
        "# 4. Make a conjecture about what underlying property of the functions or their graphs caused some to converge faster than others.\n",
        "  # The constant in each equation could be affecting the rate of convergence. For example, my equation had a constant of -18.5 with\n",
        "  # a distance of .28, and Abbbey's equation had a constant of - 26.5 with a distance of .35. These constants and distances are more \n",
        "  # similar than the constants and distances of the other equations in the group. This may indicate that there may be a \"sweet spot\" \n",
        "  # in terms of the constants helping to determine the rate of convergence. \n",
        "\n",
        "# 5. Based off of our research, we found that if r is a fixed point and g'(r) isn't equal to 0, then there will be linear convergence. \n",
        "  # If r is a fixed point and g'(r) = 0 and g''(r) â‰  0, then there will be quadratic convergence. This difference in convergence \n",
        "  # types may affect how fast the method converges. It is possible that the constant may affect the rate of convergence. In our research, we \n",
        "  # identified a table that displayed the general trend that as the constant, becomes more negative, there may be more iterations required \n",
        "  # for the method to converge to the value of the true root. \n",
        "\n",
        "# SOURCES\n",
        "# Soram, R., Takhellambam, S., Yaikhom, S., Khomdram, M., Singh, S., &amp; Roy, S. (2013). On the Rate of Convergence of Newton-Raphson Method. Retrieved February 08, 2021, from http://theijes.com/papers/v2-i11/Part.1/B02110105012.pdf\n",
        "# Senning, J. (2007). Computing and Estimating the Rate of Convergence. Retrieved February 08, 2021, from http://www.math-cs.gordon.edu/courses/ma342/handouts/rate.pdf\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.2768426322817925\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}